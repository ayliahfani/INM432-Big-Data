{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 1 - Word Counting with Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayliahfani/INM432-Big-Data/blob/main/Lab%201%3A%20Word%20Counting%20with%20Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJWSvSKOl1c"
      },
      "source": [
        "# Lab Sheet 1: PySpark Demo and Word Counting with Spark\n",
        "\n",
        "To get you started, we'll walk you through a bit of Colab specific Python and some PySpark code, and then we'll do the classic word count example, followed by some tasks for you to try.\n",
        "\n",
        "**Please run through the notebook cell by cell (using 'run' above or 'shift-return' on the keyboard).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95B3FYvqPya6"
      },
      "source": [
        "##Preliminaries: Preparing Colab and Spark\n",
        "1.   When you open this notebook from the shared \"Big_Data\" folder, you **don't have write access**. When you save it, a **copy** will be created in the **folder \"Colab Notebooks\"**.\n",
        "2.   The code below will **mount Google Drive** as a **directory** in the file system (the machine is a virtual Linux box). You will be asked to **authorise** this and provide an authentication code available through a **link**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8edGFcfkPx50"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpO-PaoGUID4"
      },
      "source": [
        "Next, we check if we can read the `Big_Data` folder. If the command below fails, go back to the shared [`Big_Data`]() folder and click on **\"Add to My Drive\"** in the folder menu, as displayed in the picture.\n",
        "![](https://drive.google.com/uc?id=10hMDK56MCfu_eI633vwJJHYbWSUNY-5L)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bn28SFjUHC_"
      },
      "source": [
        "%ls \"/content/drive/My Drive/Big_Data/data/hamlet.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpUd_fHJRInX"
      },
      "source": [
        "Next, we **install Spark** (may take a minute or two). This will need to be done **every time a new machine is created**. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S9ShIHjSlDS"
      },
      "source": [
        "%cd\n",
        "!apt-get update -qq\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!tar -xzf spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/root/spark-3.0.1-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmJvtPxjOl1e"
      },
      "source": [
        "## Part 1 - Demo: Apache Spark API with PySpark\n",
        "\n",
        "Basically there are **2 APIs** available in Apache Spark - **RDD** (Resilient Distributed Datasets) and **DataFrame** (extended by Dataset in Scala and Java). In this lab we will look at RDDs and Dataframes in Python. \n",
        "\n",
        "For **more information** on the **Spark framework** - visit (https://spark.apache.org).\n",
        "For more information on the **Pyspark API** - visit (https://spark.apache.org/docs/latest/api/python/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmoNsdqQOl1f"
      },
      "source": [
        "### 1) Access to Spark\n",
        "\n",
        "We start by cretaing a **SparkContext**, normally called **`sc`**. \n",
        "We use that to create RDDs and a **SparkSession** object (for DataFrames), often just called **`spark`**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12aW4ncAOl1h"
      },
      "source": [
        "import pyspark\n",
        "# get a spark context\n",
        "sc = pyspark.SparkContext.getOrCreate()\n",
        "print(sc)\n",
        "# get the context\n",
        "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
        "print(spark) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctb3qeAkOl1m"
      },
      "source": [
        "### 2) RDD Creation\n",
        "\n",
        "There are **multiple** ways to **create RDDs**. \n",
        "\n",
        "One is to **parallelise** a **Python object** that exists in your driver process (i.e. the process that runs this notebook), which we will do in the next cell. \n",
        "\n",
        "Anoter way is to create an **RDD** is by referencing an **external dataset** such as a shared filesystem, HDFS, HBase, or just data source offering a Hadoop InputFormat. This is what we will be using later in this lab (further down)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phwtMBs2Ol1p"
      },
      "source": [
        "# Create an RDD from a Python object in this process (the \"driver\").\n",
        "# The parallelize function  creating the \"numbers\" RDD\n",
        "data = [1,2,3,4,5]\n",
        "firstRDD = sc.parallelize(data)\n",
        "print(firstRDD)\n",
        "print(firstRDD.getStorageLevel())\n",
        "print(firstRDD.getNumPartitions())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx75RI9mOl1s"
      },
      "source": [
        "This RDD lives now on as many worker machines as are available and as are deemed useful by Spark. \n",
        "\n",
        "We can see that **`firstRDD`** has only been replicated once, which is because we only have a single machine running Spark. There are 2 partitions, so we can have 2 parallel threads "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDnZCdSTOl1t"
      },
      "source": [
        "### 3) RDD operations\n",
        "RDDs have two kinds of operations: ***Transformations*** and ***Actions***.\n",
        "\n",
        "***Transformations*** create a new RDD by applying a function to the items in the RDD. The function will be registered with the new RDD, but only be applied when needed. This is called ***lazy evaluation***.\n",
        "\n",
        "***Actions*** produce some output from the data. An *Action* will trigger the execution of all *Transformations* registered with the RDD.\n",
        "\n",
        "Here are some examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q25kDkt9Ol1v"
      },
      "source": [
        "def myfun(x):\n",
        "  return x+3\n",
        "# lambda function: x -> x+3\n",
        "# RDD2 = firstRDD.map(lambda x:x+3)\n",
        "RDD2 = firstRDD.map(myfun)  \n",
        "print(RDD2)\n",
        "# nothing happened to far, as there is no action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhIIytM9bSvm"
      },
      "source": [
        "If the functions are short (one expression, to be exact), it can be more convenient to write a **lambda** expression, that creates an **anonymous function**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEtzNnm4bZfe"
      },
      "source": [
        "RDD3 = firstRDD.map(lambda x:x+3) # this is the same as using myfun \n",
        "print(RDD3)\n",
        "# nothing happened to far, as there is no action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xp2aTh8Ol15"
      },
      "source": [
        "So far, the function we created with the lambda has not actually been executed. \n",
        "\n",
        "Next we use **`collect`**, which is an **action** and therefore triggers the execution. \n",
        "**`collect`**returns the items in the RDD back into this local driver process in a Python array.](https://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdaNRc0SOl15"
      },
      "source": [
        "a = RDD2.collect() \n",
        "print(a)\n",
        "\n",
        "b = RDD3.collect() \n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXmtXG0hOl19"
      },
      "source": [
        "As we can seee above, *myfun* (RDD2) and the *lambda x: x+3* (RDD3) have the same effect.\n",
        "\n",
        "Look here for more information about the functions provided by the RDD class: (https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUl3upk0Ol1-"
      },
      "source": [
        "### 4) Dataframes \n",
        "\n",
        "**Dataframes** are a **more structured** form of storage than RDDs and similar to Pandas dataframes.  \n",
        "\n",
        "Let us see how to create and use dataframes. There are three ways of **creating a dataframe**:\n",
        "1. from an existing RDD.\n",
        "2. form and external data source, e.g., loading the data from JSON or CSV files.\n",
        "3. programmatically specifying schema and data. \n",
        "\n",
        "Here is an example for option a). We use the *Row* class to create structured data rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSIzFqAQOl1_"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "dataList = [('Anne',21),('Bob',22),('Carl',29),('Daisy',36)] # our data as a list\n",
        "rdd = sc.parallelize(dataList) # RDD from the list\n",
        "peopleRDD = rdd.map(lambda x: Row(name=x[0], age=int(x[1]))) # RDD\n",
        "peopleDF = spark.createDataFrame(peopleRDD) \n",
        "print(peopleDF)\n",
        "peopleDF.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsGAOxX2Ol2G"
      },
      "source": [
        "## Part 2: Classic Word Count\n",
        "\n",
        "We will now do the **classic word count example** for the MapReduce pattern.\n",
        "\n",
        "We will apply it to the text of Sheakespeare's play *Hamlet*. To access that, you should have added the `Big_Data` folder to your own Google Drive. Double check that this part of the **preliminaries** section above was executed correctly, otherwise go back and fix this first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eWjGlkbOl2H"
      },
      "source": [
        "### 1) Load the data\n",
        "First we need to load the **text into an RDD** (the second method of creating an RDD as mentioned above). \n",
        "\n",
        "We need to specify the path, and we can read directly from the shared Big_Data directory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzdsuhxtOl2H"
      },
      "source": [
        "filepath = \"/content/drive/My Drive/Big_Data/data/hamlet.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5q8UiCtOl2J"
      },
      "source": [
        "You can read the file into an RDD with **`textFile`**. The RDD then contains as items the **lines of the text**. **`take(3)`** then gives us the first 3 lines.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM_vQDsGOl2J"
      },
      "source": [
        "lineRDD = sc.textFile(filepath)\n",
        "lineRDD.take(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab6vKwgvOl2L"
      },
      "source": [
        "### 2) Split lines into words\n",
        "\n",
        "In order to count the words, we need to split the lines into words. We can do that using the **`split`** function of the Python String class to separate at each space. \n",
        "\n",
        "The map function replaces each item with a new one. In this case, our **`lambda`** returns an array of words (provided by `split(' ')`). However, we want to create one item per word. Therefore, we need to use a function called **`flatMap`** that creates a new RDD item for every item in the array returned by the lambda.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwI2cDlSOl2L"
      },
      "source": [
        "wordRDD = lineRDD.flatMap(lambda x: x.split(' '))\n",
        "wordRDD.take(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDuHh_ocOl2N"
      },
      "source": [
        "We  then **map** the words to **tuples** of the form *(word, 1)*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PXZXAGpOl2O"
      },
      "source": [
        "word1RDD = wordRDD.map(lambda x: (x, 1))\n",
        "word1RDD.take(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BplN07GOl2P"
      },
      "source": [
        "### 3) Count by reducing\n",
        "For Spark, the first part in each tuple is the '**key**'. Now we can use **`reduceByKey()`** to add the 1s, giving the number of occurences per word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRFaQARsOl2Q"
      },
      "source": [
        "wordCountRDD = word1RDD.reduceByKey(lambda x,y: x+y )\n",
        "wordCountRDD.take(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G38rUrlVOl2S"
      },
      "source": [
        "### 4) Filtering \n",
        "\n",
        "There are many empty strings returned by the splitting. We can remove them by **filtering**.\n",
        "**`filter()`** keeps only the items where the test function (specified by our lambda) returns `true`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ9mBefpOl2S"
      },
      "source": [
        "wordFilteredRDD = wordRDD.filter(lambda x: len(x)>0)\n",
        "word1RDD = wordFilteredRDD.map(lambda x: (x, 1))\n",
        "wordCountRDD = word1RDD.reduceByKey(lambda x,y: x+y )\n",
        "wcList = wordCountRDD.collect()\n",
        "print(wcList[1:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyubLNalOl2W"
      },
      "source": [
        "## Part 3: Tasks for you to work on\n",
        "\n",
        "Based on the examples above, you can now try and write some code yourself.  Look for the lines starting with **>>>**. You neeed to fix them by writing your own code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bubScba3vLNG"
      },
      "source": [
        "### 0) Warm-up: Take a shortcut\n",
        "\n",
        "We can take a shortcut and use a **ready-made function** **`countByValue()`**, which does the same as we did above without us specifying a lambda for addition (https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.countByValue)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTJdSqcMveUt"
      },
      "source": [
        "wordFilteredRDD = wordRDD.filter(lambda x: len(x)>0)\n",
        ">>> wcList = # your code goes here, use countByValue on wordFilteredRDD to create wcList.\n",
        "print(wcList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LY4f9UwOl2W"
      },
      "source": [
        "### 1) Better splitting \n",
        "\n",
        "Currently our 'words' can contain punctuation, because only spaces are removed. A better way to split is using regular expressions  [Python's 're' package](https://docs.python.org/3.5/library/re.html?highlight=regular%20expressions). **`re.split('\\W+', 'my. test. string!')`** does a good job. Try it out below by fixing the line that starts with '>>>'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuQDbhOAOl2X"
      },
      "source": [
        "import re\n",
        ">>> wordRDD = lineRDD.flatMap(lambda x: ...) # apply re.split('\\W+', string) here\n",
        "wordFilteredRDD = wordRDD.filter(lambda x: len(x)>0) # filtering\n",
        "wordFilteredRDD.take(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktBnkI5TOl2Y"
      },
      "source": [
        "### 2) Use lower case\n",
        "\n",
        "**Convert all strings** to lower case (using **`.lower()`** provided by the Python string class), so that 'Test' and 'test' count as the same word. Package it into one a tuple of the form (word,1) in the same call."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBqa2EQjOl2Y"
      },
      "source": [
        ">>> wordLowerRDD = wordFilteredRDD.map(lambda x: ... )\n",
        "wordLowerRDD.take(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIVwV56lOl2a"
      },
      "source": [
        "word1LowerRDD = wordLowerRDD.map(lambda x: (x, 1))\n",
        "wordCountLowerRDD = word1LowerRDD.reduceByKey(lambda x,y: x+y) # we can now get better word count results\n",
        "wordCountLowerRDD.take(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7jgnYSVOl2b"
      },
      "source": [
        "### 3) Filter rare words\n",
        "\n",
        "Add a filtering step call **remove all words with less than 5 occurrences**. This can be useful to if you want to identify common topics in documents, where very rare words can be misleading. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTd9S_y6Ol2b"
      },
      "source": [
        "# the trick here is to apply the lambda only to the second part of each item, i.e. x[1] \n",
        ">>> freqWordsRDD = wordCountRDD.filter(lambda x:  ... ) # tip: filter keeps the elements for which the lambda function returns true\n",
        "freqWordsRDD.take(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IypcO9SPOl2d"
      },
      "source": [
        "### 4) List only stopwords\n",
        "\n",
        "**Stopwords** are frequent words that are not topic-specifc (articles, pronouns, ...). Removing stopwords can be useful in recognising the topic of a document. Removing non-stopwords can be useful for recognising the style of an author. \n",
        "\n",
        "Below is a small list of stopwords. Filter so that you keep the tuples where the **first element is in the stopword list**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T85Nx6ZuOl2e"
      },
      "source": [
        "stopWordList = ['the','a','in','of','on','at','for','by','I','you','me'] \n",
        ">>> stopWordsRDD = freqWordsRDD.filter(lambda x:  ) # the 1st part of the tuple should be in the list "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKVS18B9Ol2f"
      },
      "source": [
        "We can now list the results (there are only a few words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsX57MXHOl2g"
      },
      "source": [
        "output = stopWordsRDD.collect() \n",
        "for (word, count) in output:\n",
        "    print(\"%s: %i\" % (word, count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQcQSKw0Ol2k"
      },
      "source": [
        "## Reading\n",
        "\n",
        "Read chapter 1 of Lescovec et al (2014), \"Mining of Massive Datasets\", and work out the answers to exercise 1.2.1 on page 7 and 1.3.1 and 1.3.2 on page 15. If you have time, start reading chapter 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBMQj71VOl2k"
      },
      "source": [
        "## Spark @home (optional)\n",
        "\n",
        "You can try and install Spark on your own laptop or desktop, using the instructions provided on Moodle."
      ]
    }
  ]
}